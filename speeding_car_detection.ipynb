{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# *** Video Configuration ***\n",
    "CLIPS_FOR_TESTING = [\n",
    "    './data/clip1.MOV',\n",
    "    './data/clip2.MOV'\n",
    "]\n",
    "#VIDEO_SOURCE = \"./data/full_recording.mov\"\n",
    "VIDEO_SOURCE = CLIPS_FOR_TESTING[0]\n",
    "#VIDEO_SOURCE = CLIPS_FOR_TESTING[1]\n",
    "OUTPUT_FILE = \"./output/final_clip1.mp4\"\n",
    "CASCADE_FILE = \"cars.xml\" # for haar classifier\n",
    "\n",
    "# *** Depth calculation constants ***\n",
    "MAX_DEPTH_METERS = 21.23 \n",
    "MIN_AREA_THRESHOLD = 20000\n",
    "\n",
    "# *** Region we care about (bottom 60% b/c cars wont be in the sky)\n",
    "ROI_START_PCT = 0.4 \n",
    "\n",
    "# *** Background removal parameters ***\n",
    "MOG2_BG_REMOVER_HISTORY = 100   \n",
    "MOG2_BG_REMOVER_THRESHOLD = 40\n",
    "\n",
    "# *** Object tracking tuning params ***\n",
    "MIN_HITS = 5 \n",
    "MAX_AGE = 20 \n",
    "SPEED_LIMIT = 25         # lower speed limit so that we see violations -> it was a very snowy day so violations were limited\n",
    "BRISK_THRESHOLD = 20    # controls sensitivity: lower = more points, higher = fewer points. default = 30 ( found thru experimentation )\n",
    "HARR_SCALE_FACTOR = 1.05\n",
    "HARR_MIN_NEIGHBORS = 2\n",
    "BRISK_KEYPOINT_MIN = 5  # if we find fewer than 5 valid corners, it's probably a shadow or road patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d5fa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanTracker:\n",
    "    def __init__(self, initial_rect, id):\n",
    "        self.id = id\n",
    "        self.history = [] \n",
    "        self.speeds = deque(maxlen=10)\n",
    "        \n",
    "        self.time_since_update = 0 \n",
    "        self.hits = 0 \n",
    "        self.age = 0 \n",
    "        self.violation_recorded = False \n",
    "        \n",
    "        self.kf = cv2.KalmanFilter(4, 2)\n",
    "        self.kf.transitionMatrix = np.array([\n",
    "            [1, 0, 1, 0],\n",
    "            [0, 1, 0, 1],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ], np.float32)\n",
    "        self.kf.measurementMatrix = np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0]\n",
    "        ], np.float32)\n",
    "        self.kf.processNoiseCov = np.eye(4, dtype=np.float32) * 0.05\n",
    "        self.kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 0.1\n",
    "        \n",
    "        x, y, w, h = initial_rect\n",
    "        cx, cy = x + w/2, y + h/2\n",
    "        self.kf.statePost = np.array([[cx], [cy], [0], [0]], np.float32)\n",
    "        self.kf.errorCovPost = np.eye(4, dtype=np.float32)\n",
    "        \n",
    "        self.box_w = w\n",
    "        self.box_h = h\n",
    "\n",
    "    def predict(self):\n",
    "        prediction = self.kf.predict()\n",
    "        self.age += 1\n",
    "        if self.time_since_update > 0:\n",
    "            self.hits = 0 \n",
    "        return (prediction[0][0], prediction[1][0])\n",
    "\n",
    "    def correct(self, rect):\n",
    "        x, y, w, h = rect\n",
    "        cx, cy = x + w/2, y + h/2\n",
    "        self.kf.correct(np.array([[np.float32(cx)], [np.float32(cy)]]))\n",
    "        self.box_w = w\n",
    "        self.box_h = h\n",
    "        self.hits += 1\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.kf.statePost[0][0], self.kf.statePost[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4fdd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarTracker:\n",
    "    def __init__(self, fps):\n",
    "        self.trackers = [] \n",
    "        self.id_count = 0\n",
    "        self.fps = fps # use fps from video\n",
    "        self.dist_thresh = 100 \n",
    "\n",
    "    def update(self, detections, birds_eye_view_transformer, frame_count):\n",
    "        for trk in self.trackers: trk.predict()\n",
    "        \n",
    "        unmatched_detections = list(range(len(detections)))\n",
    "        unmatched_trackers = list(range(len(self.trackers)))\n",
    "        matches = [] \n",
    "\n",
    "        for t_idx in unmatched_trackers:\n",
    "            pred_x, pred_y = self.trackers[t_idx].get_state()\n",
    "            best_dist = self.dist_thresh\n",
    "            best_d_idx = -1\n",
    "            \n",
    "            for d_idx in unmatched_detections:\n",
    "                dx, dy, dw, dh = detections[d_idx]\n",
    "                dcx, dcy = dx + dw/2, dy + dh/2\n",
    "                \n",
    "                dist = math.hypot(dcx - pred_x, dcy - pred_y)\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best_d_idx = d_idx\n",
    "            \n",
    "            if best_d_idx != -1:\n",
    "                matches.append((t_idx, best_d_idx))\n",
    "                unmatched_detections.remove(best_d_idx)\n",
    "        \n",
    "        for t_idx, d_idx in matches:\n",
    "            self.trackers[t_idx].correct(detections[d_idx])\n",
    "            cx, cy = self.trackers[t_idx].get_state()\n",
    "            \n",
    "            self.trackers[t_idx].history.append((cx, cy, frame_count))\n",
    "            \n",
    "            if len(self.trackers[t_idx].history) >= 2:\n",
    "                self.calculate_speed(self.trackers[t_idx], birds_eye_view_transformer)\n",
    "\n",
    "        active_trackers = []\n",
    "        for i, trk in enumerate(self.trackers):\n",
    "            matched = False\n",
    "            for t_idx, _ in matches:\n",
    "                if i == t_idx: matched = True\n",
    "            if not matched: trk.time_since_update += 1 \n",
    "            if trk.time_since_update < MAX_AGE:\n",
    "                active_trackers.append(trk)\n",
    "        self.trackers = active_trackers\n",
    "\n",
    "        for d_idx in unmatched_detections:\n",
    "            new_trk = KalmanTracker(detections[d_idx], self.id_count)\n",
    "            cx, cy = new_trk.get_state()\n",
    "            new_trk.history.append((cx, cy, frame_count))\n",
    "            self.trackers.append(new_trk)\n",
    "            self.id_count += 1\n",
    "\n",
    "        return self.trackers\n",
    "\n",
    "    def calculate_speed(self, tracker, birds_eye_view_transformer):\n",
    "        history = tracker.history\n",
    "        if len(history) < 2: return\n",
    "        \n",
    "        # calc speed over 5 frames for stability\n",
    "        if len(history) >= 5:\n",
    "            idx = -5\n",
    "        else:\n",
    "            idx = 0\n",
    "        p2_cam, p1_cam = history[-1], history[idx]\n",
    "\n",
    "        # use birds eye view geometry transform\n",
    "        p1_birds_eye_view = birds_eye_view_transformer.transform_point((p1_cam[0], p1_cam[1]))\n",
    "        p2_birds_eye_view = birds_eye_view_transformer.transform_point((p2_cam[0], p2_cam[1]))\n",
    "        \n",
    "        dist_pixels = np.linalg.norm(np.array(p1_birds_eye_view) - np.array(p2_birds_eye_view))\n",
    "        dist_meters = dist_pixels * (MAX_DEPTH_METERS / birds_eye_view_transformer.birds_eye_view_height)\n",
    "\n",
    "        # we compute time based on the video fps to determine speed\n",
    "        frame_delta = p2_cam[2] - p1_cam[2]\n",
    "        if frame_delta == 0: \n",
    "            return\n",
    "        \n",
    "        time_secs = frame_delta / self.fps # frame to seconds\n",
    "        \n",
    "        if time_secs > 0:\n",
    "            speed_mph = (dist_meters / time_secs) * 2.23694\n",
    "            \n",
    "            if 1.0 < speed_mph < 100.0: # avoid random noise (like a speed over 100mph)\n",
    "                tracker.speeds.append(speed_mph)\n",
    "\n",
    "    def get_speed(self, tracker):\n",
    "        if len(tracker.speeds) > 0:\n",
    "            return np.mean(tracker.speeds)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a5f269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleClassifier:\n",
    "    def __init__(self, cascade_path):\n",
    "        if os.path.exists(cascade_path):\n",
    "            self.car_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "        else: \n",
    "            print(\"CRITICAL WARNING: HAAR file not found.\")\n",
    "            self.car_cascade = None\n",
    "        self.brisk = cv2.BRISK_create(BRISK_THRESHOLD)\n",
    "            \n",
    "    def classify(self, frame, roi_box, enable_harr=False):\n",
    "        x, y, w, h = roi_box\n",
    "        H, W = frame.shape[:2]\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        w, h = min(w, W - x), min(h, H - y)\n",
    "        \n",
    "        # if frame is super small, its noise. quick sanity check.\n",
    "        if w < 20 or h < 20: \n",
    "            return \"noise\"\n",
    "        if (w / float(h)) < 0.8: \n",
    "            return \"noise\"\n",
    "        \n",
    "        # extract ONLY our region of interest, the car cant be in the sky\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        if roi.size == 0: return \"noise\"\n",
    "        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # this checks if the ROI contains a shape matching the cars.xml haar model provided by OpenCV\n",
    "        # first pass, to avoid having to do heavy brisk check on everything\n",
    "        if enable_harr and self.car_cascade:\n",
    "            # scaleFactor=1.05: Scans image at 5% scale increments (more accurate, slower)\n",
    "            # minNeighbors=2: Requires at least 2 overlapping detections to confirm\n",
    "            cars = self.car_cascade.detectMultiScale(gray_roi, scaleFactor=HARR_SCALE_FACTOR,minNeighbors=HARR_MIN_NEIGHBORS)\n",
    "\n",
    "            if len(cars) == 0: # if harr sees nothing, imeediately reject as noise\n",
    "                return \"noise\"\n",
    "        \n",
    "        # BRISK Check\n",
    "        keypoints = self.brisk.detect(gray_roi, None)   # shadows are smooth (low keypoints), cars have grilles/lights (high keypoints)\n",
    "\n",
    "        if len(keypoints) < BRISK_KEYPOINT_MIN: # if > 5 keypoints, likely NOT a car\n",
    "            return \"noise\"\n",
    "\n",
    "        return \"car\"\n",
    "\n",
    "    def draw_speed_limit_overlay(self, frame):\n",
    "        H, W = frame.shape[:2]\n",
    "        w_sign, h_sign = 100, 120\n",
    "        x1 = 20\n",
    "        y1 = H - h_sign - 80\n",
    "        x2, y2 = x1 + w_sign, y1 + h_sign\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 2)\n",
    "        cv2.putText(frame, \"SPEED\", (x1 + 28, y1 + 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1)\n",
    "        cv2.putText(frame, \"LIMIT\", (x1 + 32, y1 + 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1)\n",
    "        text = str(SPEED_LIMIT)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        scale = 1.5\n",
    "        thickness = 3\n",
    "        \n",
    "        (text_w, text_h), _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "        text_x = x1 + (w_sign - text_w) // 2\n",
    "        text_y = y1 + 95  \n",
    "        \n",
    "        cv2.putText(frame, text, (text_x, text_y), font, scale, (0, 0, 0), thickness)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef5233ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdsEyeViewTransformation:\n",
    "    def __init__(self, width, height):\n",
    "        w, h = width, height\n",
    "        self.src_points = np.float32([\n",
    "            [w * 0.20, h * ROI_START_PCT],  \n",
    "            [w * 0.80, h * ROI_START_PCT],  \n",
    "            [w, h],                \n",
    "            [0, h]                 \n",
    "        ])\n",
    "        self.birds_eye_view_width = 300\n",
    "        self.birds_eye_view_height = 600 \n",
    "        self.dst_points = np.float32([[0, 0], [self.birds_eye_view_width, 0], [self.birds_eye_view_width, self.birds_eye_view_height], [0, self.birds_eye_view_height]])\n",
    "        self.M = cv2.getPerspectiveTransform(self.src_points, self.dst_points)\n",
    "    \n",
    "    def transform_point(self, point):\n",
    "        p = np.array([[[point[0], point[1]]]], dtype=np.float32)\n",
    "        return cv2.perspectiveTransform(p, self.M)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcaf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_system():\n",
    "    cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "    if not cap.isOpened(): \n",
    "        print(\"CRITICAL failed to read video in\")\n",
    "        return\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        print(\"CRITICAL failed to read video in\")\n",
    "        return\n",
    "    \n",
    "    birds_eye_view = BirdsEyeViewTransformation(frame.shape[1], frame.shape[0])\n",
    "    tracker = CarTracker(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    classifier = VehicleClassifier(CASCADE_FILE) # initialize classifier\n",
    "    \n",
    "    # remove background\n",
    "    object_detector = cv2.createBackgroundSubtractorMOG2(history=MOG2_BG_REMOVER_HISTORY, varThreshold=MOG2_BG_REMOVER_THRESHOLD)\n",
    "\n",
    "    # output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    out = cv2.VideoWriter(OUTPUT_FILE, fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "    total_violations = 0\n",
    "\n",
    "    # stores as a tuple: (frame_number, x, y)\n",
    "    recent_violations = deque(maxlen=5) \n",
    "    frame_count = 0\n",
    "\n",
    "    print(f\"Beginning procesing pipeline, saving output to {OUTPUT_FILE}\")\n",
    "\n",
    "    frame_count=0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            print(\"CRITICAL failed to read in frame\")\n",
    "            break\n",
    "        frame_count+=1\n",
    "        height, width, _ = frame.shape\n",
    "        \n",
    "        # region of interest mask\n",
    "        mask_roi = np.zeros((height, width), dtype=np.uint8)\n",
    "        cv2.rectangle(mask_roi, (0, int(height * ROI_START_PCT)), (width, height), 255, -1)\n",
    "        roi_frame = cv2.bitwise_and(frame, frame, mask=mask_roi)\n",
    "\n",
    "        # background mask\n",
    "        mask_motion = object_detector.apply(roi_frame)\n",
    "        _, mask_motion = cv2.threshold(mask_motion, 250, 255, cv2.THRESH_BINARY)\n",
    "        mask_motion = cv2.morphologyEx(mask_motion, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "        mask_motion = cv2.morphologyEx(mask_motion, cv2.MORPH_CLOSE, np.ones((20,20),np.uint8))\n",
    "        \n",
    "        # grab edges\n",
    "        contours, _ = cv2.findContours(mask_motion, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # prep the overlay view\n",
    "        edges_color = cv2.cvtColor(cv2.Canny(roi_frame, 50, 150), cv2.COLOR_GRAY2BGR)\n",
    "        mask_color = cv2.cvtColor(mask_motion, cv2.COLOR_GRAY2BGR)\n",
    "        stage3_frame = roi_frame.copy() \n",
    "        kalman_frame = roi_frame.copy() \n",
    "        \n",
    "        boxes_for_nms = []\n",
    "        confidences = []\n",
    "\n",
    "        # grab initial contour candidates (just edges w/ a min area)\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > MIN_AREA_THRESHOLD:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                if (y + h//2) < height * ROI_START_PCT: \n",
    "                    continue\n",
    "                boxes_for_nms.append([x, y, w, h])\n",
    "                confidences.append(float(area))\n",
    "                cv2.rectangle(stage3_frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        # nonmaximal suppresion\n",
    "        stage4_frame = roi_frame.copy()\n",
    "        indices = cv2.dnn.NMSBoxes(boxes_for_nms, confidences, score_threshold=MIN_AREA_THRESHOLD, nms_threshold=0.3)\n",
    "        confirmed_cars = []\n",
    "        \n",
    "        # BRISK to verify car classiifcaiton\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                box = boxes_for_nms[i]\n",
    "                if classifier.classify(frame, box) == \"car\":\n",
    "                    confirmed_cars.append(box)\n",
    "                    cv2.rectangle(stage4_frame, (box[0], box[1]), (box[0]+box[2], box[1]+box[3]), (0, 255, 0), 2) # visualize brisk\n",
    "\n",
    "        # update tracking\n",
    "        active_trackers = tracker.update(confirmed_cars, birds_eye_view, frame_count)\n",
    "\n",
    "        stage5_frame = np.zeros_like(roi_frame)\n",
    "        for trk in active_trackers:\n",
    "            if trk.hits >= MIN_HITS:\n",
    "                points = trk.history\n",
    "                if len(points) > 1:\n",
    "                    for j in range(1, len(points)):\n",
    "                        pt1 = (int(points[j - 1][0]), int(points[j - 1][1]))\n",
    "                        pt2 = (int(points[j][0]), int(points[j][1]))\n",
    "                        cv2.line(stage5_frame, pt1, pt2, (0, 255, 255), 2)\n",
    "                        \n",
    "            # draw Kalman filter boxes to show tracking\n",
    "            cx, cy = trk.get_state()\n",
    "            kx, ky = int(cx - trk.box_w/2), int(cy - trk.box_h/2)\n",
    "            if trk.hits >= MIN_HITS:\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                color = (0, 255, 255)\n",
    "            cv2.rectangle(kalman_frame, (kx, ky), (kx+trk.box_w, ky+trk.box_h), color, 2)\n",
    "\n",
    "        # dashboard overlay composition\n",
    "        dashboard_h = int(height * 0.2)\n",
    "        dashboard_w = int(width / 6)\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        frames = [edges_color, mask_color, stage3_frame, stage4_frame, stage5_frame, kalman_frame]\n",
    "        titles = [\"1. EDGES\", \"2. BACKGROUND MASK + ROI\", \"3. RAW\", \"4. NMS + BRISK\", \"5. PATH\", \"6. KALMAN TRACKING\"]\n",
    "        colors = [(0,255,0), (0,255,0), (0,255,0), (0,255,0), (0,255,0), (0,255,0)]\n",
    "\n",
    "        # draw overlays\n",
    "        for i, (f, t, c) in enumerate(zip(frames, titles, colors)):\n",
    "            resized = cv2.resize(f, (dashboard_w, dashboard_h))\n",
    "            cv2.putText(resized, t, (10, 20), font, 1.2, c, 1)\n",
    "            frame[0:dashboard_h, dashboard_w*i:dashboard_w*(i+1)] = resized\n",
    "            cv2.line(frame, (dashboard_w*(i+1), 0), (dashboard_w*(i+1), dashboard_h), (255, 255, 0), 2)\n",
    "        cv2.rectangle(frame, (0,0), (width, dashboard_h), (255, 255, 0), 2)\n",
    "\n",
    "        # add violation overlay (RED SPEEDING box)\n",
    "        for trk in active_trackers:\n",
    "            if trk.hits < MIN_HITS: \n",
    "                # track must exist for a certain duration (protect against noisy detections)\n",
    "                continue\n",
    "            speed = tracker.get_speed(trk)\n",
    "            cx, cy = trk.get_state()\n",
    "            x, y = int(cx - trk.box_w/2), int(cy - trk.box_h/2)\n",
    "            \n",
    "            is_speeding = speed > SPEED_LIMIT\n",
    "\n",
    "            is_duplicate = False\n",
    "            if is_speeding and not trk.violation_recorded:\n",
    "                for v_frame, v_x, v_y in recent_violations:\n",
    "                    if (frame_count - v_frame) < 1000:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                        # dist = math.hypot(cx - v_x, cy - v_y)\n",
    "                        # if dist < 1000: \n",
    "                        #     is_duplicate = True\n",
    "                        #     break\n",
    "            \n",
    "            if is_speeding and not trk.violation_recorded:\n",
    "                total_violations += 1\n",
    "\n",
    "            if is_speeding:\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                color = (0, 255, 0)\n",
    "            label = f\"{int(speed)} MPH\"\n",
    "            if is_speeding: \n",
    "                label += \" SPEEDING!\"\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + trk.box_w, y + trk.box_h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), font, 1.5, color, 2)\n",
    "\n",
    "            # save side profile (metadata) of the violaton.\n",
    "            if is_speeding and not trk.violation_recorded:\n",
    "                filename = f\"offenders/violation_{total_violations}.jpg\"\n",
    "                cv2.imwrite(filename, frame)\n",
    "                trk.violation_recorded = True \n",
    "\n",
    "        # update global stats that need to persist\n",
    "        cv2.rectangle(frame, (20, height - 60), (400, height - 10), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, f\"TOTAL VIOLATIONS: {total_violations}\", (25, height - 25), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "        \n",
    "        frame = classifier.draw_speed_limit_overlay(frame)\n",
    "        \n",
    "        cv2.imshow(\"Speed Detection\", frame)\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(30) == 27: \n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cf910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning procesing pipeline, saving output to ./output/final_clip1.mp4\n"
     ]
    }
   ],
   "source": [
    "run_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780b955-1e7b-4c41-9b10-ffc299d44979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
